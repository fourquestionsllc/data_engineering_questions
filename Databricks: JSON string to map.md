To convert a column from a **JSON string** to a **Map type** in a Databricks table (using Spark SQL or PySpark), you can use the built-in function `from_json()`.

---

### ðŸ”¹ **Using SQL**
If your column is called `json_col` and contains JSON strings like:
```json
{"key1":"value1", "key2":"value2"}
```

You can convert it like this:
```sql
SELECT
  from_json(json_col, 'map<string,string>') AS json_map
FROM
  your_table
```

This returns a column of **Map<String, String>** type.

---

### ðŸ”¹ **Using PySpark**
```python
from pyspark.sql.functions import from_json, col
from pyspark.sql.types import MapType, StringType

df = spark.table("your_table")

# Convert JSON string column to Map
df_with_map = df.withColumn(
    "json_map",
    from_json(col("json_col"), MapType(StringType(), StringType()))
)

df_with_map.show(truncate=False)
```

---

### âœ… Notes:
- If your JSON values are not all strings (e.g., numbers or nested structures), adjust the schema accordingly.
- `from_json()` will return `null` if the string is not valid JSON.
